# Parallel Computing Usage Guide

## Overview

ê¸°ì¡´ ì½”ë“œëŠ” **ìˆœì°¨ì (sequential)** ì‹¤í–‰ë§Œ ì§€ì›í–ˆìŠµë‹ˆë‹¤. ì´ì œ **parallel computing**ì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ atlas ì‹œë®¬ë ˆì´ì…˜ì„ ë™ì‹œì— ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

## ğŸš€ Performance Comparison

### Sequential (ê¸°ì¡´ ë°©ì‹)
```bash
# ìˆœì°¨ì ìœ¼ë¡œ í•˜ë‚˜ì”© ì‹¤í–‰ - SLOW!
python src/simulation.py --nodes 48 --timepoints 375 --tr 0.8   # ~5 min
python src/simulation.py --nodes 68 --timepoints 375 --tr 0.8   # ~8 min
python src/simulation.py --nodes 360 --timepoints 375 --tr 0.8  # ~20 min

# Total: ~33 minutes
```

### Parallel (ìƒˆë¡œìš´ ë°©ì‹) âš¡
```bash
# 3ê°œ atlasë¥¼ ë™ì‹œì— ì‹¤í–‰ - FAST!
python run_parallel_atlases.py --timepoints 375 --tr 0.8 --workers 3

# Total: ~20 minutes (ê°€ì¥ ì˜¤ë˜ ê±¸ë¦¬ëŠ” 360-node ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„)
# Speedup: ~1.65x faster!
```

## ğŸ“Š Available Scripts

### 1. `run_parallel_atlases.py` - Multiple Atlas Parallel Execution

ì—¬ëŸ¬ atlas êµ¬ì„±(48, 68, 360 nodes)ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.

**Basic Usage:**
```bash
# Default: 48, 68, 360 nodesë¥¼ ëª¨ë‘ ë³‘ë ¬ ì‹¤í–‰
python run_parallel_atlases.py

# íŠ¹ì • íŒŒë¼ë¯¸í„° ì§€ì •
python run_parallel_atlases.py --timepoints 375 --tr 0.8 --workers 3

# ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©
python run_parallel_atlases.py --model kuramoto --workers 3

# íŠ¹ì • atlasë§Œ ì‹¤í–‰
python run_parallel_atlases.py --atlases 48 68 --workers 2
```

**Arguments:**
- `--timepoints`: ì‹œë®¬ë ˆì´ì…˜ timepoints (default: 375)
- `--tr`: Repetition time in seconds (default: 0.8)
- `--model`: Neural mass model (`wong_wang`, `kuramoto`, `generic_2d_oscillator`)
- `--output`: ì¶œë ¥ ë””ë ‰í† ë¦¬ (default: `results`)
- `--workers`: ë³‘ë ¬ worker ìˆ˜ (default: 3)
- `--atlases`: ì‹¤í–‰í•  atlas êµ¬ì„± (default: 48 68 360)

**Output:**
```
results/
â”œâ”€â”€ sim_48nodes_375tp_TR0.8_neural.npy
â”œâ”€â”€ sim_48nodes_375tp_TR0.8_bold.npy
â”œâ”€â”€ sim_48nodes_375tp_TR0.8_metadata.json
â”œâ”€â”€ sim_68nodes_375tp_TR0.8_neural.npy
â”œâ”€â”€ sim_68nodes_375tp_TR0.8_bold.npy
â”œâ”€â”€ sim_68nodes_375tp_TR0.8_metadata.json
â”œâ”€â”€ sim_360nodes_375tp_TR0.8_neural.npy
â”œâ”€â”€ sim_360nodes_375tp_TR0.8_bold.npy
â”œâ”€â”€ sim_360nodes_375tp_TR0.8_metadata.json
â””â”€â”€ parallel_atlas_summary.json  # ë³‘ë ¬ ì‹¤í–‰ ìš”ì•½
```

### 2. `src/parallel_simulation.py` - Multiple Subject Parallel Execution

ì—¬ëŸ¬ subjectë¥¼ ë³‘ë ¬ë¡œ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤ (ëŒ€ê·œëª¨ ì—°êµ¬ìš©).

**Usage:**
```bash
# 10,000ëª…ì˜ subject ì‹œë®¬ë ˆì´ì…˜ (32 CPUs ì‚¬ìš©)
python src/parallel_simulation.py --subjects 10000 --nodes 48 --workers 32

# ì‘ì€ ê·œëª¨ í…ŒìŠ¤íŠ¸
python src/parallel_simulation.py --subjects 100 --nodes 48 --workers 8
```

**Arguments:**
- `--subjects`: ì‹œë®¬ë ˆì´ì…˜í•  subject ìˆ˜ (default: 10000)
- `--nodes`: Atlas êµ¬ì„± (48 or 360)
- `--workers`: ë³‘ë ¬ worker ìˆ˜ (default: 32)
- `--start`: ì‹œì‘ subject ID (ì¬ê°œìš©)

**Output:**
```
results/parallel_sims/
â”œâ”€â”€ subject_00000/
â”‚   â”œâ”€â”€ sim_48nodes_375tp_TR0.8_neural.npy
â”‚   â”œâ”€â”€ sim_48nodes_375tp_TR0.8_bold.npy
â”‚   â””â”€â”€ sim_48nodes_375tp_TR0.8_metadata.json
â”œâ”€â”€ subject_00001/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ...
â””â”€â”€ simulation_summary.json  # ì „ì²´ batch ìš”ì•½
```

## ğŸ’» System Requirements

### Recommended Configuration

**For Atlas Parallel Execution (run_parallel_atlases.py):**
- CPU cores: 3+ (ê° atlasë‹¹ 1 core)
- RAM: 8-16 GB
- Disk space: ~5 GB (3 atlases)

**For Subject Parallel Execution (parallel_simulation.py):**
- CPU cores: 8-32 (ë” ë§ì„ìˆ˜ë¡ ë¹ ë¦„)
- RAM: 32-64 GB (32 workers ê¸°ì¤€)
- Disk space: ëŒ€ê·œëª¨ (10,000 subjects = ~500 GB)

### CPU Core Usage

í˜„ì¬ ì‹œìŠ¤í…œì˜ CPU ì •ë³´ í™•ì¸:
```bash
python -c "from multiprocessing import cpu_count; print(f'Available CPU cores: {cpu_count()}')"
```

## ğŸ”§ Python API Usage

### Example 1: Run All Atlases in Parallel

```python
from run_parallel_atlases import run_all_atlases_parallel

# Run 48, 68, 360 nodes in parallel
results = run_all_atlases_parallel(
    num_timepoints=375,
    TR=0.8,
    model_type='wong_wang',
    output_prefix='results',
    num_workers=3
)

# Check results
for r in results:
    if r['status'] == 'success':
        print(f"{r['num_nodes']} nodes: {r['elapsed_time_minutes']:.1f} min")
```

### Example 2: Parameter Sweep

```python
from run_parallel_atlases import run_parameter_sweep

# Test multiple parameters in parallel
results = run_parameter_sweep(
    node_configs=[48, 68],
    timepoints_list=[375, 750],
    TR_list=[0.8, 1.0],
    model_types=['wong_wang', 'kuramoto'],
    output_prefix='results/parameter_sweep',
    num_workers=8
)

# Total combinations: 2 nodes Ã— 2 timepoints Ã— 2 TRs Ã— 2 models = 16 simulations
# With 8 workers: ~2-3x faster than sequential
```

### Example 3: Multiple Subjects

```python
from src.parallel_simulation import run_parallel_simulations

# Simulate 1000 subjects with 48 nodes each
results = run_parallel_simulations(
    num_subjects=1000,
    num_nodes=48,
    num_timepoints=375,
    TR=0.8,
    model_type='wong_wang',
    output_dir='results/1000subjects',
    num_workers=16
)

# Check completion rate
successful = sum(1 for r in results if r['status'] == 'success')
print(f"Completed: {successful}/1000 subjects")
```

## ğŸ“ˆ Performance Tips

### 1. Optimal Worker Count

**Atlas Parallel (`run_parallel_atlases.py`):**
- 3 workers for 3 atlases (48, 68, 360)
- More workers don't help since only 3 simulations

**Subject Parallel (`parallel_simulation.py`):**
- Use `CPU cores - 1` to leave system responsive
- Example: 32-core machine â†’ use 28-30 workers

### 2. Memory Considerations

**Per simulation memory usage:**
- 48 nodes: ~200 MB
- 68 nodes: ~400 MB
- 360 nodes: ~1.5 GB

**For parallel execution:**
- 3 workers (all atlases): ~2 GB total
- 16 workers (48 nodes): ~3-4 GB total
- 32 workers (48 nodes): ~6-8 GB total

### 3. Disk I/O

ë³‘ë ¬ ì‹¤í–‰ ì‹œ ë””ìŠ¤í¬ ì“°ê¸°ê°€ ë³‘ëª©ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- SSD ì‚¬ìš© ê¶Œì¥
- ê° simulationì„ ë‹¤ë¥¸ ë””ë ‰í† ë¦¬ì— ì €ì¥
- ë„¤íŠ¸ì›Œí¬ ë“œë¼ì´ë¸Œë³´ë‹¤ ë¡œì»¬ ë””ìŠ¤í¬ ì‚¬ìš©

## ğŸ› Troubleshooting

### Issue: "Too many workers" warning

```
Warning: Requested 32 workers but only 8 CPUs available
Using 8 workers instead
```

**Solution:** ì‹œìŠ¤í…œ CPU ìˆ˜ì— ë§ê²Œ `--workers` ì¡°ì •

### Issue: Out of memory

```
MemoryError: Unable to allocate array
```

**Solution:**
1. Worker ìˆ˜ ì¤„ì´ê¸°: `--workers 4`
2. ë” ì‘ì€ atlas ì‚¬ìš©: `--atlases 48 68` (360 ì œì™¸)
3. Timepoints ì¤„ì´ê¸°: `--timepoints 200`

### Issue: Simulations hanging

**Solution:**
1. Ctrl+Cë¡œ ì¤‘ë‹¨
2. Worker ìˆ˜ ì¤„ì´ê¸°
3. í•œ ë²ˆì— í•˜ë‚˜ì”© í…ŒìŠ¤íŠ¸:
   ```bash
   python src/simulation.py --nodes 48 --timepoints 375 --tr 0.8
   ```

## ğŸ“ Summary

| Method | Script | Use Case | Speedup |
|--------|--------|----------|---------|
| **Sequential** | `src/simulation.py` | Single atlas | 1x (baseline) |
| **Atlas Parallel** | `run_parallel_atlases.py` | Multiple atlases | ~1.5-2x |
| **Subject Parallel** | `src/parallel_simulation.py` | Large studies | ~8-16x (with 16-32 workers) |

## ğŸ¯ Recommended Workflow

### For Exploratory Analysis (ì†Œê·œëª¨)
```bash
# Test single atlas first
python src/simulation.py --nodes 68 --timepoints 375 --tr 0.8

# Then run all atlases in parallel
python run_parallel_atlases.py --timepoints 375 --tr 0.8 --workers 3
```

### For Large-Scale Studies (ëŒ€ê·œëª¨)
```bash
# Run many subjects in parallel
python src/parallel_simulation.py --subjects 10000 --nodes 48 --workers 32
```

### For Parameter Exploration (íŒŒë¼ë¯¸í„° íƒìƒ‰)
```python
from run_parallel_atlases import run_parameter_sweep

run_parameter_sweep(
    node_configs=[48, 68, 360],
    timepoints_list=[375],
    TR_list=[0.5, 0.8, 1.0, 1.5],
    model_types=['wong_wang'],
    num_workers=8
)
```

---

**Questions?** Check the main README.md or CLAUDE.md for more details about the simulation framework.
